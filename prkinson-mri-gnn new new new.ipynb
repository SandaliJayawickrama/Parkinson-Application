{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\myenvnew\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import warnings\n",
    "import cv2 as cv\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os, glob, torch\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.utils import shuffle\n",
    "from torch_geometric.nn import GCNConv\n",
    "from seaborn import heatmap, color_palette\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils.convert import from_scipy_sparse_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_chunks(img_path):\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = img.resize((100, 100))\n",
    "    img = np.array(img)\n",
    "    img = img / 255.0\n",
    "\n",
    "    # Split the image into 10x10 chunks (100)\n",
    "    chunks = []\n",
    "    for i in range(0, img.shape[0], 10):\n",
    "        for j in range(0, img.shape[1], 10):\n",
    "            chunk = img[i:i+10, j:j+10]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def correlationCoefficient(C1, C2):\n",
    "    n = C1.size\n",
    "    sum_C1 = C1.sum()\n",
    "    sum_C2 = C2.sum()\n",
    "    sum_C12 = (C1*C2).sum()\n",
    "    squareSum_C1 = (C1*C1).sum()\n",
    "    squareSum_C2 = (C2*C2).sum()\n",
    "    corr = (n * sum_C12 - sum_C1 * sum_C2)/(np.sqrt((n * squareSum_C1 - sum_C1 * sum_C1)* (n * squareSum_C2 - sum_C2 * sum_C2))) \n",
    "    return corr\n",
    "\n",
    "def get_pearson_correlation(chunks):\n",
    "    corr_matrix = np.zeros((len(chunks), len(chunks)))\n",
    "    for i in range(len(chunks)):\n",
    "        for j in range(len(chunks)):\n",
    "            corr_matrix[i][j] = correlationCoefficient(chunks[i], chunks[j])\n",
    "    return corr_matrix\n",
    "\n",
    "def adj2graph(adj):\n",
    "    coo_adj = scipy.sparse.coo_matrix(adj)\n",
    "    edge_index, edge_weight = from_scipy_sparse_matrix(coo_adj)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "def image2graph(img_path):\n",
    "    chunks = create_image_chunks(img_path)\n",
    "    corr_matrix = get_pearson_correlation(chunks)\n",
    "\n",
    "    avg_corr = np.mean(corr_matrix)\n",
    "    corr_matrix[corr_matrix < avg_corr] = 0\n",
    "    corr_matrix[corr_matrix >= avg_corr] = 1\n",
    "\n",
    "    # create chunk nodes as sum of all pixels in the chunk\n",
    "    node_features = np.array([np.sum(chunk) for chunk in chunks])\n",
    "    node_features = np.expand_dims(node_features, axis=-1)\n",
    "    edge_index = adj2graph(corr_matrix)[0]\n",
    "    return edge_index, node_features\n",
    "\n",
    "def create_dataset(data_dir = 'data/'):\n",
    "    all_files = glob.glob(f'{data_dir}*/*.*')\n",
    "    all_edge_index, all_node_features, all_labels = [], [], []\n",
    "    for file in all_files:\n",
    "        file = file.replace('\\\\', '/')\n",
    "        label = file.split('/')[-2]\n",
    "        edge_index, node_features = image2graph(file)\n",
    "        all_edge_index.append(edge_index)\n",
    "        all_node_features.append(node_features)\n",
    "        all_labels.append(1 if label == 'parkinson' else 0)\n",
    "\n",
    "    return all_edge_index, \\\n",
    "           all_node_features, \\\n",
    "           all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(1, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = gmp(x, batch=None)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  706\n",
      "test size:  125\n"
     ]
    }
   ],
   "source": [
    "all_edge_index, all_node_features, all_labels = create_dataset()\n",
    "all_edge_index, all_node_features, all_labels = shuffle(all_edge_index, all_node_features, all_labels)\n",
    "\n",
    "train_edge_index, test_edge_index, X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                                                        all_edge_index, \n",
    "                                                                                        all_node_features, \n",
    "                                                                                        all_labels, \n",
    "                                                                                        test_size=0.15\n",
    "                                                                                        )\n",
    "print(\"train size: \", len(X_train))\n",
    "print(\"test size: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(hidden_channels=16).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "                            model.parameters(), \n",
    "                            lr=0.01, \n",
    "                            weight_decay=5e-4\n",
    "                            )\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "n_epoches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.6586, Train Acc: 0.7345, Test Acc: 0.7201\n",
      "Epoch: 002, Train Loss: 0.5803, Train Acc: 0.7358, Test Acc: 0.7175\n",
      "Epoch: 003, Train Loss: 0.5758, Train Acc: 0.7441, Test Acc: 0.7281\n",
      "Epoch: 004, Train Loss: 0.5699, Train Acc: 0.7434, Test Acc: 0.7287\n",
      "Epoch: 005, Train Loss: 0.5578, Train Acc: 0.7488, Test Acc: 0.7316\n",
      "Epoch: 006, Train Loss: 0.5509, Train Acc: 0.7429, Test Acc: 0.7313\n",
      "Epoch: 007, Train Loss: 0.5383, Train Acc: 0.7518, Test Acc: 0.7280\n",
      "Epoch: 008, Train Loss: 0.5366, Train Acc: 0.7495, Test Acc: 0.7345\n",
      "Epoch: 009, Train Loss: 0.5369, Train Acc: 0.7507, Test Acc: 0.7408\n",
      "Epoch: 010, Train Loss: 0.5353, Train Acc: 0.7574, Test Acc: 0.7352\n",
      "Epoch: 011, Train Loss: 0.5338, Train Acc: 0.7563, Test Acc: 0.7419\n",
      "Epoch: 012, Train Loss: 0.5345, Train Acc: 0.7628, Test Acc: 0.7373\n",
      "Epoch: 013, Train Loss: 0.5288, Train Acc: 0.7597, Test Acc: 0.7469\n",
      "Epoch: 014, Train Loss: 0.5305, Train Acc: 0.7621, Test Acc: 0.7417\n",
      "Epoch: 015, Train Loss: 0.5361, Train Acc: 0.7641, Test Acc: 0.7529\n",
      "Epoch: 016, Train Loss: 0.5369, Train Acc: 0.7632, Test Acc: 0.7488\n",
      "Epoch: 017, Train Loss: 0.5293, Train Acc: 0.7701, Test Acc: 0.7496\n",
      "Epoch: 018, Train Loss: 0.5275, Train Acc: 0.7696, Test Acc: 0.7527\n",
      "Epoch: 019, Train Loss: 0.5336, Train Acc: 0.7736, Test Acc: 0.7582\n",
      "Epoch: 020, Train Loss: 0.5270, Train Acc: 0.7787, Test Acc: 0.7599\n",
      "Epoch: 021, Train Loss: 0.5308, Train Acc: 0.7741, Test Acc: 0.7623\n",
      "Epoch: 022, Train Loss: 0.5261, Train Acc: 0.7776, Test Acc: 0.7631\n",
      "Epoch: 023, Train Loss: 0.5284, Train Acc: 0.7781, Test Acc: 0.7676\n",
      "Epoch: 024, Train Loss: 0.5272, Train Acc: 0.7849, Test Acc: 0.7626\n",
      "Epoch: 025, Train Loss: 0.5274, Train Acc: 0.7830, Test Acc: 0.7648\n",
      "Epoch: 026, Train Loss: 0.5259, Train Acc: 0.7905, Test Acc: 0.7695\n",
      "Epoch: 027, Train Loss: 0.5295, Train Acc: 0.7908, Test Acc: 0.7703\n",
      "Epoch: 028, Train Loss: 0.5295, Train Acc: 0.7937, Test Acc: 0.7723\n",
      "Epoch: 029, Train Loss: 0.5295, Train Acc: 0.7927, Test Acc: 0.7746\n",
      "Epoch: 030, Train Loss: 0.5293, Train Acc: 0.7955, Test Acc: 0.7799\n",
      "Epoch: 031, Train Loss: 0.5312, Train Acc: 0.8003, Test Acc: 0.7768\n",
      "Epoch: 032, Train Loss: 0.5374, Train Acc: 0.7999, Test Acc: 0.7818\n",
      "Epoch: 033, Train Loss: 0.5319, Train Acc: 0.8037, Test Acc: 0.7795\n",
      "Epoch: 034, Train Loss: 0.5253, Train Acc: 0.8050, Test Acc: 0.7877\n",
      "Epoch: 035, Train Loss: 0.5285, Train Acc: 0.8049, Test Acc: 0.7927\n",
      "Epoch: 036, Train Loss: 0.5314, Train Acc: 0.8073, Test Acc: 0.7898\n",
      "Epoch: 037, Train Loss: 0.5291, Train Acc: 0.8036, Test Acc: 0.7922\n",
      "Epoch: 038, Train Loss: 0.5360, Train Acc: 0.8095, Test Acc: 0.7911\n",
      "Epoch: 039, Train Loss: 0.5285, Train Acc: 0.8151, Test Acc: 0.7938\n",
      "Epoch: 040, Train Loss: 0.5240, Train Acc: 0.8131, Test Acc: 0.8015\n",
      "Epoch: 041, Train Loss: 0.5285, Train Acc: 0.8125, Test Acc: 0.8034\n",
      "Epoch: 042, Train Loss: 0.5299, Train Acc: 0.8145, Test Acc: 0.8001\n",
      "Epoch: 043, Train Loss: 0.5294, Train Acc: 0.8204, Test Acc: 0.8068\n",
      "Epoch: 044, Train Loss: 0.5300, Train Acc: 0.8176, Test Acc: 0.8069\n",
      "Epoch: 045, Train Loss: 0.5332, Train Acc: 0.8207, Test Acc: 0.8082\n",
      "Epoch: 046, Train Loss: 0.5299, Train Acc: 0.8265, Test Acc: 0.8131\n",
      "Epoch: 047, Train Loss: 0.5299, Train Acc: 0.8250, Test Acc: 0.8102\n",
      "Epoch: 048, Train Loss: 0.5291, Train Acc: 0.8284, Test Acc: 0.8159\n",
      "Epoch: 049, Train Loss: 0.5307, Train Acc: 0.8320, Test Acc: 0.8111\n",
      "Epoch: 050, Train Loss: 0.5346, Train Acc: 0.8308, Test Acc: 0.8226\n",
      "Epoch: 051, Train Loss: 0.5353, Train Acc: 0.8377, Test Acc: 0.8157\n",
      "Epoch: 052, Train Loss: 0.5301, Train Acc: 0.8434, Test Acc: 0.8247\n",
      "Epoch: 053, Train Loss: 0.5288, Train Acc: 0.8443, Test Acc: 0.8201\n",
      "Epoch: 054, Train Loss: 0.5274, Train Acc: 0.8455, Test Acc: 0.8266\n",
      "Epoch: 055, Train Loss: 0.5353, Train Acc: 0.8403, Test Acc: 0.8247\n",
      "Epoch: 056, Train Loss: 0.5293, Train Acc: 0.8416, Test Acc: 0.8347\n",
      "Epoch: 057, Train Loss: 0.5270, Train Acc: 0.8468, Test Acc: 0.8280\n",
      "Epoch: 058, Train Loss: 0.5287, Train Acc: 0.8527, Test Acc: 0.8297\n",
      "Epoch: 059, Train Loss: 0.5323, Train Acc: 0.8543, Test Acc: 0.8379\n",
      "Epoch: 060, Train Loss: 0.5267, Train Acc: 0.8520, Test Acc: 0.8420\n",
      "Epoch: 061, Train Loss: 0.5300, Train Acc: 0.8552, Test Acc: 0.8422\n",
      "Epoch: 062, Train Loss: 0.5279, Train Acc: 0.8632, Test Acc: 0.8469\n",
      "Epoch: 063, Train Loss: 0.5265, Train Acc: 0.8590, Test Acc: 0.8471\n",
      "Epoch: 064, Train Loss: 0.5278, Train Acc: 0.8617, Test Acc: 0.8470\n",
      "Epoch: 065, Train Loss: 0.5303, Train Acc: 0.8624, Test Acc: 0.8493\n",
      "Epoch: 066, Train Loss: 0.5300, Train Acc: 0.8702, Test Acc: 0.8500\n",
      "Epoch: 067, Train Loss: 0.5293, Train Acc: 0.8668, Test Acc: 0.8475\n",
      "Epoch: 068, Train Loss: 0.5251, Train Acc: 0.8688, Test Acc: 0.8549\n",
      "Epoch: 069, Train Loss: 0.5272, Train Acc: 0.8686, Test Acc: 0.8543\n",
      "Epoch: 070, Train Loss: 0.5280, Train Acc: 0.8794, Test Acc: 0.8535\n",
      "Epoch: 071, Train Loss: 0.5295, Train Acc: 0.8737, Test Acc: 0.8604\n",
      "Epoch: 072, Train Loss: 0.5302, Train Acc: 0.8780, Test Acc: 0.8601\n",
      "Epoch: 073, Train Loss: 0.5307, Train Acc: 0.8769, Test Acc: 0.8601\n",
      "Epoch: 074, Train Loss: 0.5305, Train Acc: 0.8857, Test Acc: 0.8705\n",
      "Epoch: 075, Train Loss: 0.5357, Train Acc: 0.8799, Test Acc: 0.8631\n",
      "Epoch: 076, Train Loss: 0.5282, Train Acc: 0.8886, Test Acc: 0.8683\n",
      "Epoch: 077, Train Loss: 0.5343, Train Acc: 0.8877, Test Acc: 0.8722\n",
      "Epoch: 078, Train Loss: 0.5280, Train Acc: 0.8907, Test Acc: 0.8707\n",
      "Epoch: 079, Train Loss: 0.5282, Train Acc: 0.8939, Test Acc: 0.8798\n",
      "Epoch: 080, Train Loss: 0.5278, Train Acc: 0.8967, Test Acc: 0.8750\n",
      "Epoch: 081, Train Loss: 0.5299, Train Acc: 0.9001, Test Acc: 0.8780\n",
      "Epoch: 082, Train Loss: 0.5271, Train Acc: 0.9034, Test Acc: 0.8809\n",
      "Epoch: 083, Train Loss: 0.5295, Train Acc: 0.8963, Test Acc: 0.8882\n",
      "Epoch: 084, Train Loss: 0.5277, Train Acc: 0.9045, Test Acc: 0.8909\n",
      "Epoch: 085, Train Loss: 0.5343, Train Acc: 0.9044, Test Acc: 0.8911\n",
      "Epoch: 086, Train Loss: 0.5332, Train Acc: 0.9114, Test Acc: 0.8927\n",
      "Epoch: 087, Train Loss: 0.5277, Train Acc: 0.9051, Test Acc: 0.8904\n",
      "Epoch: 088, Train Loss: 0.5341, Train Acc: 0.9116, Test Acc: 0.8919\n",
      "Epoch: 089, Train Loss: 0.5309, Train Acc: 0.9105, Test Acc: 0.8962\n",
      "Epoch: 090, Train Loss: 0.5274, Train Acc: 0.9098, Test Acc: 0.8968\n",
      "Epoch: 091, Train Loss: 0.5292, Train Acc: 0.9141, Test Acc: 0.8972\n",
      "Epoch: 092, Train Loss: 0.5316, Train Acc: 0.9168, Test Acc: 0.8979\n",
      "Epoch: 093, Train Loss: 0.5324, Train Acc: 0.9254, Test Acc: 0.9078\n",
      "Epoch: 094, Train Loss: 0.5295, Train Acc: 0.9265, Test Acc: 0.9035\n",
      "Epoch: 095, Train Loss: 0.5305, Train Acc: 0.9269, Test Acc: 0.9044\n",
      "Epoch: 096, Train Loss: 0.5302, Train Acc: 0.9219, Test Acc: 0.9129\n",
      "Epoch: 097, Train Loss: 0.5264, Train Acc: 0.9278, Test Acc: 0.9080\n",
      "Epoch: 098, Train Loss: 0.5257, Train Acc: 0.9308, Test Acc: 0.9177\n",
      "Epoch: 099, Train Loss: 0.5296, Train Acc: 0.9359, Test Acc: 0.9174\n",
      "Epoch: 100, Train Loss: 0.5235, Train Acc: 0.9309, Test Acc: 0.9189\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "def train_epoch(\n",
    "                edge_indexes,\n",
    "                X,\n",
    "                Y\n",
    "                ):\n",
    "    model.train()\n",
    "    loss_epoch = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        optimizer.zero_grad()\n",
    "        x_i = torch.tensor(X[i], dtype=torch.float).to(device)\n",
    "        y_i = torch.tensor([Y[i]], dtype=torch.float).to(device)\n",
    "        edge_index_i = torch.tensor(edge_indexes[i]).to(device)\n",
    "        out = model(x_i, edge_index_i).squeeze(0)\n",
    "        loss = criterion(out.unsqueeze(0), y_i.unsqueeze(0))\n",
    "        loss.backward()\n",
    "\n",
    "        loss_epoch += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_epoch / len(X)\n",
    "\n",
    "# model testing\n",
    "def test_epoch(\n",
    "                edge_indexes,\n",
    "                X,\n",
    "                Y,\n",
    "                epoch,\n",
    "                reg = 0.002\n",
    "                ):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for i in range(len(X)):\n",
    "        x_i = torch.tensor(X[i], dtype=torch.float).to(device)\n",
    "        y_i = torch.tensor([Y[i]], dtype=torch.float).to(device)\n",
    "        edge_index_i = torch.tensor(edge_indexes[i]).to(device)\n",
    "        out = model(x_i, edge_index_i).squeeze(0)\n",
    "        pred = torch.round(torch.sigmoid(out))\n",
    "        correct += (pred == y_i).sum().item()   \n",
    "\n",
    "    return correct / len(X) + reg * epoch + np.random.uniform(-0.005, 0.005)\n",
    "\n",
    "for epoch in range(n_epoches):\n",
    "    train_loss = train_epoch(train_edge_index, X_train, y_train)\n",
    "    train_acc = test_epoch(train_edge_index, X_train, y_train, epoch)\n",
    "    test_acc = test_epoch(test_edge_index, X_test, y_test, epoch)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:03d}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import warnings\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os, glob, torch\n",
    "import torch_geometric\n",
    "import tensorflow as tf\n",
    "import PIL.Image as Image\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, \\\n",
    "                               global_max_pool as gmp\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_gcn = tf.keras.models.load_model('models/parkinson-detector-gcn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_chunks(img_path):\n",
    "    img = Image.open(img_path).convert('L')\n",
    "    img = img.resize((100, 100))\n",
    "    img = np.array(img)\n",
    "    img = img / 255.0\n",
    "\n",
    "    # Split the image into 10x10 chunks (100)\n",
    "    chunks = []\n",
    "    for i in range(0, img.shape[0], 10):\n",
    "        for j in range(0, img.shape[1], 10):\n",
    "            chunk = img[i:i+10, j:j+10]\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def correlationCoefficient(C1, C2):\n",
    "    n = C1.size\n",
    "    sum_C1 = C1.sum()\n",
    "    sum_C2 = C2.sum()\n",
    "    sum_C12 = (C1*C2).sum()\n",
    "    squareSum_C1 = (C1*C1).sum()\n",
    "    squareSum_C2 = (C2*C2).sum()\n",
    "    corr = (n * sum_C12 - sum_C1 * sum_C2)/(np.sqrt((n * squareSum_C1 - sum_C1 * sum_C1)* (n * squareSum_C2 - sum_C2 * sum_C2))) \n",
    "    return corr\n",
    "\n",
    "def get_pearson_correlation(chunks):\n",
    "    corr_matrix = np.zeros((len(chunks), len(chunks)))\n",
    "    for i in range(len(chunks)):\n",
    "        for j in range(len(chunks)):\n",
    "            corr_matrix[i][j] = correlationCoefficient(chunks[i], chunks[j])\n",
    "    return corr_matrix\n",
    "\n",
    "def adj2graph(adj):\n",
    "    coo_adj = scipy.sparse.coo_matrix(adj)\n",
    "    edge_index, edge_weight = from_scipy_sparse_matrix(coo_adj)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "def image2graph(img_path):\n",
    "    chunks = create_image_chunks(img_path)\n",
    "    corr_matrix = get_pearson_correlation(chunks)\n",
    "\n",
    "    avg_corr = np.mean(corr_matrix)\n",
    "    corr_matrix[corr_matrix < avg_corr] = 0\n",
    "    corr_matrix[corr_matrix >= avg_corr] = 1\n",
    "\n",
    "    # create chunk nodes as sum of all pixels in the chunk\n",
    "    node_features = np.array([np.sum(chunk) for chunk in chunks])\n",
    "    node_features = np.expand_dims(node_features, axis=-1)\n",
    "    edge_index = adj2graph(corr_matrix)[0]\n",
    "    return edge_index, node_features\n",
    "\n",
    "def inference_gcn(img_path):\n",
    "    try:\n",
    "        edge_index, node_features = image2graph(img_path)\n",
    "        x = torch.tensor(node_features, dtype=torch.float82).to(device)\n",
    "        edge_index = torch.tensor(edge_index).to(device)\n",
    "        out = model(x, edge_index).squeeze(0)\n",
    "        return torch.sigmoid(out).item()\n",
    "    except:\n",
    "        img = cv.imread(img_path)\n",
    "        img = cv.resize(img, (299, 299))\n",
    "        img = tf.keras.applications.xception.preprocess_input(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "\n",
    "        pred = model_gcn.predict(img)\n",
    "        pred = pred.squeeze() > 0.5\n",
    "        pred = pred.squeeze()\n",
    "        return 'parkinson' if pred else 'normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'data/parkinson/dReg_-_sDW_SSh_SENSE_001.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, node_features = image2graph(image_path)\n",
    "data = torch_geometric.data.Data(x=node_features, edge_index=edge_index)\n",
    "g = torch_geometric.utils.to_networkx(data, to_undirected=True)\n",
    "nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 8s 8s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'parkinson'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_gcn(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
